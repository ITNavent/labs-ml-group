{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pytorch usa tensors que son como Numpy Arrays pero que permiten usar GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5000, 3.0000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creo un tensor a partir de un array\n",
    "x = torch.tensor([5.5, 3])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creo un tensor nuevo lleno de unos de tamaño 5x3 y de tipo double\n",
    "x = x.new_ones(5, 3, dtype=torch.double)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1481,  0.8755,  0.1508],\n",
       "        [-2.4927, -0.5325,  0.5597],\n",
       "        [-2.0421,  1.4451,  0.5499],\n",
       "        [ 1.5889,  1.3446,  0.8928],\n",
       "        [-1.5053,  1.1538, -1.4614]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creo un tensor nuevo de randoms de las mismas dimensiones que x y type double\n",
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.1481,  2.8755,  2.1508],\n",
       "        [-0.4927,  1.4675,  2.5597],\n",
       "        [-0.0421,  3.4451,  2.5499],\n",
       "        [ 3.5889,  3.3446,  2.8928],\n",
       "        [ 0.4947,  3.1538,  0.5386]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x.add_(y) le suma y a x y reemplaza a x\n",
    "# En general en Pytorch, _ al final del método significa que reemplaza el valor original\n",
    "# También, las operaciones permiten cierta flexibilidad con las dimensiones. Notar que\n",
    "# las dimensiones no son iguales pero la suma se puede hacer. Esto se llama \"broadcasting\".\n",
    "y = torch.ones(5,1)\n",
    "x.add_(y)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Any operation that mutates a tensor in-place is post-fixed with an _. For example: x.copy_(y), x.t_(), will change x\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8942,  0.2657, -0.2630,  1.7027,  0.2278, -0.8865, -0.0218, -0.2573,\n",
      "         0.1125,  0.4335, -0.7348, -0.0514, -0.5023, -0.8460, -0.3186, -1.0333])\n",
      "tensor([[ 0.8942,  0.2657, -0.2630,  1.7027,  0.2278, -0.8865, -0.0218, -0.2573],\n",
      "        [ 0.1125,  0.4335, -0.7348, -0.0514, -0.5023, -0.8460, -0.3186, -1.0333]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#// view() cambia las dimensiones de un tensor. \n",
    "#En el ejemplo, x es de 4x4, y es de 1x16 y z es de 2x8.\n",
    "#(the size -1 is inferred from other dimensions)\n",
    "\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(y), print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9758, 0.9461, 0.7154, 0.2642],\n",
      "        [0.5464, 0.9209, 0.2802, 0.2258],\n",
      "        [0.4051, 0.2930, 0.6805, 0.7186],\n",
      "        [0.5035, 0.8637, 0.4811, 0.7705]])\n",
      "0.9757923483848572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# item() extrae el valor de un tensor de size 1.\n",
    "# En el ejemplo, x es de 4x4, y es un tensor que \n",
    "# contiene el a_11 de x, y z es el valor a_11\n",
    "\n",
    "x = torch.rand(4,4)\n",
    "y = x[0,0]\n",
    "z = y.item()\n",
    "\n",
    "print(x), print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6285, 0.3848, 0.2676, 0.0810],\n",
      "        [0.6853, 0.5309, 0.6695, 0.5338],\n",
      "        [0.3306, 0.0384, 0.1694, 0.4270],\n",
      "        [0.2405, 0.1109, 0.7073, 0.7763]])\n",
      "<class 'torch.Tensor'>\n",
      "[[0.628523   0.38484317 0.2676205  0.08104604]\n",
      " [0.68534005 0.5308684  0.66948014 0.533823  ]\n",
      " [0.3305546  0.03840983 0.16943514 0.4270267 ]\n",
      " [0.24050122 0.11094588 0.70730984 0.77629733]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None, None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy() permite pasar de Tensor a Numpy Array. \n",
    "# y no es una copia de x, ES x visto como numpy array. \n",
    "# IMPORTANTE: Cualquier cambio en x afecta también a y.\n",
    "\n",
    "x = torch.rand(4,4)\n",
    "y = x.numpy()\n",
    "print(x), print(type(x)), print(y), print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"torch.Tensor is the central class of the package. If you set its attribute .requires_grad as True, it starts to track all operations on it. When you finish your computation you can call .backward() and have all the gradients computed automatically. The gradient for this tensor will be accumulated into .grad attribute.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Como requires_grad = True, se van a trackear todas las operaciones sobre x y sus \"hijos\".\n",
    "# grad_fn es da la información de qué tipo de función generó al tensor, en el caso de y,\n",
    "# la función que la generó fue una suma.\n",
    "x = torch.tensor([1.], requires_grad=True)\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([27.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Hacemos algunas cuentas mas...\n",
    "z = y * y * 3\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward() computa el gradiente hasta el origen, en este caso hasta x y lo guarda\n",
    "# en x.grad\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# la función que toma x y devuelve z es 3(x+2)^2, cuyo gradiente respecto a x es\n",
    "# 6(x+2). Como x es 1, esto devuelve 18.\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vemos el caso de una campo escalar (R³ -> R)\n",
    "x = torch.tensor([1.,2.,3.], requires_grad = True)\n",
    "y = x*10\n",
    "z = y.mean()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En el caso de un un campo escalar de dimensión 3, el gradiente es un vector\n",
    "# de dimension 3\n",
    "z.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3067, 0.0051, 0.0071], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vemos el caso de un campo vectorial (R³ -> R³)\n",
    "x = torch.rand(3, requires_grad = True)\n",
    "y = x*x\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tratamos de hacer backpropagation no vamos a poder porque sólo lo puede hacer\n",
    "# para funciones con output de dimensión 1. Si ejecutamos y.backward() nos dirá:\n",
    "# RuntimeError: grad can be implicitly created only for scalar outputs\n",
    "# Como y.backward() calcula el jacobiano de la función que transforma x en y, hay que\n",
    "# hay que pasarle un vector para que multiplique al jacobiano y devuelva otro vector.\n",
    "\n",
    "l = torch.tensor([1.,1.,2.])  # Este vector me lo inventé\n",
    "y.backward(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1076, 0.1427, 0.3382])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y ahora sí tengo el vector x.grad que es la derivada de la función que transforma a x\n",
    "# en y en la dirección del vector l.\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An nn.Module contains layers, and a method forward(input)that returns the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)  # Toma un channel, devuelve 6 con kernel 3x3\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3) # Toma 6 channels, devuelve 16 con 3x3 kernel\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x)) # aplana la imagen a un vector\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn only supports mini-batches. The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
    "\n",
    "For example, nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width.\n",
    "\n",
    "If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 3, 3])\n",
      "torch.Size([16, 6, 3, 3])\n",
      "torch.Size([120, 576])\n"
     ]
    }
   ],
   "source": [
    "# net.parameters() devuelve los parametros o pesos de la red.\n",
    "params = list(net.parameters())\n",
    "print(params[0].size())    # Acá tengo los pesos de la primer capa convolucional\n",
    "print(params[2].size())    # Acá tengo los pesos de la segunda capa convolucional\n",
    "print(params[4].size())    # Acá tengo los pesos de la primer capa lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0144,  0.0273, -0.0275, -0.1011,  0.0435,  0.0849, -0.0759,  0.0107,\n",
      "          0.1155, -0.0927]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Probamos mandarle un input al azar.\n",
    "# Recordar que el input que acepta tiene que tener las dimensiones de:\n",
    "# nSamples x nChannels x Height x Width.\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)   # Lo mismo que llamar a net.forward(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero the gradient buffers of all parameters and backprops with random gradients:\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0144,  0.0273, -0.0275, -0.1011,  0.0435,  0.0849, -0.0759,  0.0107,\n",
      "          0.1155, -0.0927]], grad_fn=<AddmmBackward>)\n",
      "torch.Size([1, 10])\n",
      "\n",
      "\n",
      "tensor([-1.1819, -1.1137, -0.1241,  1.8570,  2.1434,  0.9439,  0.5108,  0.8496,\n",
      "        -1.7820, -0.1530])\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "tensor([[-1.1819, -1.1137, -0.1241,  1.8570,  2.1434,  0.9439,  0.5108,  0.8496,\n",
      "         -1.7820, -0.1530]])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "print(output)\n",
    "print(output.size())\n",
    "print(\"\\n\")\n",
    "target = torch.randn(10)     # a dummy target, for example\n",
    "print(target)\n",
    "print(target.size())\n",
    "print(\"\\n\")\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "print(target)\n",
    "print(target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6376, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7f4a9574a5c0>\n",
      "<AddmmBackward object at 0x7f4a9574a588>\n",
      "<AccumulateGrad object at 0x7f4a9574a5c0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BackPropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To backpropagate the error all we have to do is to loss.backward(). You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.\n",
    "\n",
    "Now we shall call loss.backward(), and have a look at conv1’s bias gradients before and after the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0142, -0.0011, -0.0067, -0.0109, -0.0019, -0.0094])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos de la capa 4\n",
      "tensor([-0.0699, -0.2959, -0.4610,  0.0054, -0.3026, -0.0740, -0.1181, -0.1369,\n",
      "        -0.1198,  0.0096, -0.3139,  0.0485, -0.1253, -0.0129, -0.0891, -0.2784])\n",
      "\n",
      "\n",
      "Gradiente de la capa 4\n",
      "tensor([ 0.1128,  0.4082,  0.4433,  0.0034,  0.3045,  0.2302,  0.0121,  0.1037,\n",
      "         0.0080,  0.0000,  0.2605,  0.0000,  0.1816,  0.0101, -0.0068,  0.4309])\n"
     ]
    }
   ],
   "source": [
    "# Noto que net.parameters() contiene los parametros (pesos) en .data pero también\n",
    "# contiene los gradientes en .grad\n",
    "print(\"Pesos de la capa 4\")\n",
    "print(list(net.parameters())[3].data)\n",
    "print(\"\\n\")\n",
    "print(\"Gradiente de la capa 4\")\n",
    "print(list(net.parameters())[3].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):\n",
    "\n",
    "weight = weight - learning_rate * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this, we built a small package: torch.optim that implements all these methods. Using it is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)     # Forward pass\n",
    "loss = criterion(output, target)    #Calculo el loss para este mini batch\n",
    "loss.backward()         # Calculo el backward pass\n",
    "optimizer.step()        # Actualizo los pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
